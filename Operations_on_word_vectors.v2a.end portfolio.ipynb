{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; \n",
    "            color: #FFC07F; \n",
    "            margin: 15px; \n",
    "            font-size: 1.5em; \n",
    "            display: fill; \n",
    "            border-radius: 10px; \n",
    "            border: 2px solid #FF9F00; \n",
    "            background-color: #2F2E41; \n",
    "            overflow: hidden; \n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\">\n",
    "    <center>\n",
    "        <a id=\"title\"></a>\n",
    "        <b style=\"font-size: 1.8em; color: #FFC07F;\">Operations on word vectors</b><br><br>\n",
    "        <br><br>\n",
    "        <a id=\"top\"></a>\n",
    "        <b style=\"font-size: 1.4em; color: #FFC07F;\">Table of Contents</b>\n",
    "    </center>\n",
    "    <br>\n",
    "    <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li style=\"padding: 8px 0;\">\n",
    "                <a href=\"#1\" style=\"color: #FFCC66; text-decoration: none;\">1 - Overview</a>\n",
    "            </li>\n",
    "            <ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#1-1\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">1.1. Operations on Word Vectors</a>\n",
    "                </li>\n",
    "                <ul style=\"list-style-type: none; padding-left: 10;\">\n",
    "                    <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#1-1-1\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 14px;\">1.1.1. Objectives</a>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <li style=\"padding: 8px 0;\">\n",
    "            <a href=\"#2\" style=\"color: #FFCC66; text-decoration: none;\">2 - Imports</a>\n",
    "        </li>\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li style=\"padding: 8px 0;\">\n",
    "                <a href=\"#3\" style=\"color: #FFCC66; text-decoration: none;\">3 - Data Preparation</a>\n",
    "            </li>\n",
    "            <ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#3-1\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">3.1. Loading the Word Vectors</a>\n",
    "                </li>\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#3-2\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">3.2. Embedding Vectors vs. One-Hot Vectors</a>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li style=\"padding: 8px 0;\">\n",
    "                <a href=\"#4\" style=\"color: #FFCC66; text-decoration: none;\">4. Word Analogy</a>\n",
    "            </li>\n",
    "            <ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#4-1\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">4.1. Cosine Similarity</a>\n",
    "                </li>  \n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#4-2\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">4.2. Word Analogy</a>\n",
    "                </li>  \n",
    "            </ul>\n",
    "        </ul>\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li style=\"padding: 8px 0;\">\n",
    "                <a href=\"#5\" style=\"color: #FFCC66; text-decoration: none;\">5 - Debiasing Word Vectors</a>\n",
    "            </li>\n",
    "            <ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#5-1\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">5.1. Observing Gender Bias in Word Embeddings</a>\n",
    "                </li>\n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#5-2\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">5.2. Neutralizing Bias for Non-Gender Specific Words</a>\n",
    "                </li> \n",
    "                <li style=\"padding: 8px 0;\">\n",
    "                    <a href=\"#5-3\" style=\"color: #FFCC66; text-decoration: none; font-weight: normal; font-size: 16px;\">5.3. Equalization Algorithm for Gender-Specific Words</a>\n",
    "                </li> \n",
    "            </ul>\n",
    "        </ul>\n",
    "        <li style=\"padding: 8px 0;\">\n",
    "            <a href=\"#6\" style=\"color: #FFCC66; text-decoration: none;\">6. Conclusion</a>\n",
    "        </li>\n",
    "        <li style=\"padding: 8px 0;\">\n",
    "            <a href=\"#7\" style=\"color: #FFCC66; text-decoration: none;\">7 - References</a>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: column; align-items: center; gap: 10px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:umermjd11@gmail.com\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Email-umermjd11@gmail.com-informational?style=for-the-badge&logo=gmail&logoColor=white&color=FF5722\" alt=\"Email Shield\"></a><br><br>\n",
    "    <a href=\"https://umermjd11.github.io/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Website-umermjd11.github.com-2F2E41?style=for-the-badge&logo=google-chrome&logoColor=white\" alt=\"Website Shield\"></a><br><br>\n",
    "    <a href=\"https://github.com/umermjd11\" target=\"_blank\"><img src=\"https://img.shields.io/badge/GitHub-umermjd11-181717?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub Shield\"></a><br><br>\n",
    "    <a href=\"https://kaggle.com/umermjd11\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Kaggle-umermjd11-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white\" alt=\"Kaggle Shield\"></a><br><br>\n",
    "    <a href=\"https://linkedin.com/in/umermjd11\" target=\"_blank\"><img src=\"https://img.shields.io/badge/LinkedIn-umermjd11-blue?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn Shield\"></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">1 - Overview</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "\n",
    "## 1.1. Operations on Word Vectors\n",
    "\n",
    "In this analysis, we examine word vectors, exploring how pre-trained word embeddings can be applied to measure semantic similarity, solve analogy problems, and adjust for biases inherent in language representations. \n",
    "\n",
    "Word embeddings are computationally intensive to train from scratch, so we utilize pre-trained vectors to streamline our study.\n",
    "\n",
    "<a id=\"1-1-1\"></a>\n",
    "\n",
    "### 1.1.1. Objectives:\n",
    "\n",
    "- Load pre-trained word vectors and calculate similarity through cosine similarity.\n",
    "- Utilize word embeddings to address analogy-based queries, such as \"Man is to Woman as King is to ______.\"\n",
    "- Adjust embeddings to mitigate gender biases present in word representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">2 - Imports</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:13:14.473255: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 16:13:14.651023: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 16:13:14.747496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731427994.871207    2159 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731427994.920321    2159 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 16:13:15.355809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">3 - Data Preparation</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1\"></a>\n",
    "\n",
    "## 3.1. Loading the Word Vectors\n",
    "\n",
    "In this section, we employ 50-dimensional GloVe vectors to represent words, leveraging pre-trained embeddings that capture semantic relationships efficiently. The following cell loads the `word_to_vec_map`, a dictionary that maps words to their respective vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, we have:\n",
    "\n",
    "- **`words`**: a set containing all words in the vocabulary.\n",
    "- **`word_to_vec_map`**: a dictionary that maps each word to its GloVe vector representation.\n",
    "\n",
    "<a id=\"3-2\"></a>\n",
    "\n",
    "## 3.2. Embedding Vectors vs. One-Hot Vectors\n",
    "\n",
    "One-hot vectors are limited in capturing semantic similarity, as every one-hot vector is equidistant from any other, lacking nuanced meaning. In contrast, embedding vectors, such as GloVe, encode semantic information, making them valuable for tasks involving word similarity.\n",
    "\n",
    "Next, we will explore how GloVe vectors can be used to measure the similarity between two words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">4 - Word Analogy</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-1\"></a>\n",
    "\n",
    "## 4.1. Cosine Similarity\n",
    "\n",
    "To quantify the similarity between two words, we calculate the cosine similarity between their embedding vectors. Given vectors $u$ and $v$, the cosine similarity is defined as:\n",
    "\n",
    "$$\\text{CosineSimilarity}(u, v) = \\frac {u \\cdot v} {||u||_2 ||v||_2} = \\cos(\\theta) \\tag{1}$$\n",
    "\n",
    "where:\n",
    "- $u \\cdot v$ is the dot product of vectors $u$ and $v$,\n",
    "- $||u||_2$ is the Euclidean norm of vector $u$,\n",
    "- $\\theta$ is the angle between $u$ and $v$.\n",
    "\n",
    "The cosine similarity measures the angle between two vectors:\n",
    "- When $u$ and $v$ are highly similar, cosine similarity approaches 1.\n",
    "- For dissimilar vectors, cosine similarity is lower.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "  <br>\n",
    "  <caption>\n",
    "    <font color=\"#00796b\" style=\"font-size: 1.2em; font-weight: bold;\">\n",
    "      <b>Figure 1</b>: The cosine of the angle between two vectors serves as a measure of their similarity.\n",
    "    </font>\n",
    "  </caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Task**: Implement the function `cosine_similarity()` to compute the similarity between two word vectors.\n",
    "\n",
    "**Hint**: The norm of $u$ can be computed as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2} $. Functions such as `np.dot`, `np.sum`, and `np.sqrt` may be useful for implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the dot product between u and v\n",
    "    dot = np.dot(u, v)\n",
    "    \n",
    "    # Compute the L2 norms of u and v\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets play with `cosine_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.8909038442893615\n",
      "cosine_similarity(ball, crocodile) =  0.2743924626137942\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.6751479308174202\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-2\"></a>\n",
    "\n",
    "## 4.2. Word Analogy\n",
    "\n",
    "In this section, we explore the word analogy task, where the objective is to complete sentences of the form:  \n",
    "<font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>.\n",
    "\n",
    "A classic example of this task is:  \n",
    "<font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>.\n",
    "\n",
    "To solve this task, we aim to find a word *d* such that the word vectors associated with *a*, *b*, *c*, and *d*—denoted as $e_a$, $e_b$, $e_c$, and $e_d$ respectively—satisfy the following relationship:  \n",
    "$$\n",
    "e_b - e_a \\approx e_d - e_c\n",
    "$$\n",
    "\n",
    "The similarity between the vector pairs $e_b - e_a$ and $e_d - e_c$ will be measured using cosine similarity, which allows us to quantify how closely the directional relationship between *a* and *b* matches that of *c* and *d*.\n",
    "\n",
    "**Objective**: The code below perform word analogies by identifying the word *d* that best completes each analogy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task: a is to b as c is to ____.\n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word -- the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b, and e_c\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = list(word_to_vec_map.keys())  # Convert the keys to a list to iterate over\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None\n",
    "    \n",
    "    # Create a set of input words to avoid choosing them as the result\n",
    "    input_words_set = set([word_a, word_b, word_c])\n",
    "    \n",
    "    # Loop over all words in the word vector map\n",
    "    for w in words:\n",
    "        # Skip the input words\n",
    "        if w in input_words_set:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between (e_b - e_a) and (e_w - e_c)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # Update the best_word if a higher cosine similarity is found\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "    \n",
    "    return best_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the functionality of the above code, lets execute the cell below. Please note that this process may take approximately 1–2 minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">5. Debiasing Word Vectors</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the important topic of debiasing word vectors, an area that addresses ethical considerations in natural language processing (NLP). Here, we will examine gender biases that may be reflected in word embeddings and explore algorithms designed to reduce such biases. This section not only provides insights into debiasing techniques but also helps build an intuitive understanding of what word vectors represent.\n",
    "\n",
    "To begin, we will investigate how GloVe word embeddings capture gender-related information. Specifically, we will compute a vector $g = e_{woman} - e_{man}$, where $e_{woman}$ and $e_{man}$ represent the word vectors corresponding to \"woman\" and \"man,\" respectively. The resulting vector $g$ serves as an approximate encoding of the \"gender\" concept.\n",
    "\n",
    "For a more nuanced representation, we could compute multiple vectors that reflect gender, such as $g_1 = e_{mother} - e_{father}$ and $g_2 = e_{girl} - e_{boy}$, and average them to form a composite gender vector. However, for our purposes here, the vector $g = e_{woman} - e_{man}$ will provide sufficiently accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
      " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
      "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
      "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
      "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
      " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
      " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
      "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
      " -0.04371     0.01258   ]\n"
     ]
    }
   ],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine the cosine similarity between various word vectors and the gender vector $g$. Consider the implications of positive versus negative cosine similarity values in this context. A positive cosine similarity indicates alignment with the gender direction represented by $g$, while a negative cosine similarity suggests an inverse relationship. Reflect on how these values may reveal inherent gender associations within the word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.23163356145973724\n",
      "marie 0.315597935396073\n",
      "sophie 0.31868789859418784\n",
      "ronaldo -0.31244796850329437\n",
      "priya 0.17632041839009402\n",
      "rahul -0.16915471039231716\n",
      "danielle 0.24393299216283895\n",
      "reza -0.07930429672199553\n",
      "katy 0.2831068659572615\n",
      "yasmin 0.2331385776792876\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, female first names tend to exhibit a positive cosine similarity with the constructed gender vector $g$, while male first names generally have a negative cosine similarity. This result aligns with our expectations and seems reasonable within the context of gender representation.\n",
    "\n",
    "\n",
    "The gender vector $g$, defined as $g = e_{woman} - e_{man}$, captures a directional representation of gender within the embedding space. This vector essentially encodes the semantic difference between words associated with \"woman\" and \"man,\" creating a kind of \"gender axis\" in the vector space. \n",
    "\n",
    "When we compute the cosine similarity between $g$ and other word vectors, we measure how well those words align with the concept of \"female\" (as defined by $g$) or \"male\" (as defined by the opposite direction of $g$). \n",
    "\n",
    "In practice, female-associated words—such as female first names—tend to have a positive cosine similarity with $g$. This positive similarity indicates that these word vectors are positioned closer to the \"female\" end of the gender axis in the embedding space. Conversely, male-associated words, such as male first names, often show a negative cosine similarity with $g$, suggesting that they lie in the opposite direction along this axis, closer to the \"male\" end.\n",
    "\n",
    "This alignment occurs because the word vectors for female names have likely been influenced by contexts in which they co-occur with gendered words or concepts associated with \"woman.\" Similarly, male names have likely been shaped by contexts that include associations with \"man\" or other masculine terms. As a result, the vector $g$ effectively serves as a gendered directional marker, allowing us to observe inherent gender associations in the embedding space.\n",
    "\n",
    "This phenomenon is not surprising, given that word embeddings are trained on large text corpora that reflect real-world usage patterns, including social and cultural biases. By examining the cosine similarity of various words with $g$, we can gain insight into these implicit biases within the embeddings.\n",
    "\n",
    "\n",
    "However, let us extend this analysis by examining the cosine similarity of other words with the vector $g$ to gain deeper insights into potential gender associations embedded within the word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.2769191625638267\n",
      "guns -0.1888485567898898\n",
      "science -0.06082906540929701\n",
      "arts 0.008189312385880337\n",
      "literature 0.06472504433459932\n",
      "warrior -0.20920164641125288\n",
      "doctor 0.11895289410935041\n",
      "tree -0.07089399175478091\n",
      "receptionist 0.3307794175059374\n",
      "technology -0.13193732447554302\n",
      "fashion 0.03563894625772699\n",
      "teacher 0.17920923431825664\n",
      "engineer -0.0803928049452407\n",
      "pilot 0.0010764498991916937\n",
      "computer -0.10330358873850498\n",
      "singer 0.1850051813649629\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-1\"></a>\n",
    "\n",
    "## 5.1. Observing Gender Bias in Word Embeddings\n",
    "\n",
    "Upon closer inspection, we notice some surprising—and troubling—patterns in the results. The word embeddings reflect certain gender stereotypes that are often considered unhealthy or limiting. For example, words such as \"computer\" are positioned closer to \"man,\" while words like \"literature\" are closer to \"woman.\" These associations reveal implicit biases present in the embeddings, mirroring societal stereotypes. \n",
    "\n",
    "To address this issue, we will explore a method for reducing bias in word vectors, following an algorithm proposed by [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520). This approach involves selectively modifying word embeddings to reduce unwanted gender associations. Note that certain word pairs, such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather,\" should retain their gender-specific distinctions. In contrast, other words like \"receptionist\" or \"technology\" should be neutralized to ensure they do not imply gender.\n",
    "\n",
    "### 5.2. Neutralizing Bias for Non-Gender Specific Words\n",
    "\n",
    "The neutralization process removes gender-related bias from words that should be gender-neutral. To illustrate, if we are working in a 50-dimensional embedding space, this space can be conceptually divided into two components: the **bias direction** $g$, and the remaining 49 dimensions, which we’ll refer to as $g_{\\perp}$. In linear algebra terms, $g_{\\perp}$ is the subspace orthogonal to $g$, meaning it is at a 90-degree angle to $g$. The neutralization step removes the component of a word vector in the direction of $g$, resulting in a \"debiased\" vector. \n",
    "\n",
    "For example, we could neutralize the bias in the word vector for \"receptionist\" by zeroing out its component along $g$, producing a new vector $e_{receptionist}^{debiased}$. Although $g_{\\perp}$ is 49-dimensional, the figure below illustrates this process in a simplified 2D representation for clarity.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    " <img src=\"https://raw.githubusercontent.com/umermjd11/DLC5M2A1/master/images/neutral.png\" style=\"width:800px;height:300px;\">\n",
    "  <br>\n",
    "  <caption>\n",
    "    <font color=\"#00796b\" style=\"font-size: 1.2em; font-weight: bold;\">\n",
    "      <b>Figure 2</b>: The word vector for \"receptionist,\" shown before and after applying the neutralization operation.\n",
    "    </font>\n",
    "  </caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Objective**: Implement the `neutralize()` function to remove bias from words such as \"receptionist\" or \"scientist.\" Given an input embedding $e$, use the following formulas to compute the debiased vector $e^{debiased}$:\n",
    "\n",
    "1. Compute the **bias component** of $e$ along $g$:\n",
    "   $$\n",
    "   e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\n",
    "   $$\n",
    "\n",
    "2. Subtract the bias component from $e$ to obtain the **debiased vector**:\n",
    "   $$\n",
    "   e^{debiased} = e - e^{bias\\_component}\n",
    "   $$\n",
    "\n",
    "For those familiar with linear algebra, $e^{bias\\_component}$ represents the projection of $e$ onto the direction defined by $g$. If linear algebra concepts are unfamiliar, simply focus on implementing the formulas to achieve the desired debiasing effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\"\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute the bias component\n",
    "    e_biascomponent = (np.dot(e, g) / np.linalg.norm(g)**2) * g\n",
    "    \n",
    "    # Neutralize by subtracting the bias component\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.3307794175059374\n",
      "cosine similarity between receptionist and g, after neutralizing:  -2.099120994400013e-17\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Equalization Algorithm for Gender-Specific Words\n",
    "\n",
    "In this section, we explore how debiasing can be applied to pairs of words that are gender-specific, such as \"actor\" and \"actress.\" The **equalization algorithm** is designed to make such pairs equidistant from gender-neutral words, allowing them to differ only by their gender attribute, without being influenced by gender stereotypes in other contexts.\n",
    "\n",
    "For example, consider the word \"babysit.\" Suppose \"actress\" is closer to \"babysit\" than \"actor,\" reflecting a gender stereotype in the embedding space. By applying neutralization to \"babysit,\" we can reduce this stereotype. However, neutralizing alone does not ensure that \"actor\" and \"actress\" are equidistant from \"babysit\" or other neutral words. The equalization algorithm addresses this by ensuring that pairs of gender-specific words, like \"actor\" and \"actress,\" are symmetrically positioned relative to gender-neutral terms.\n",
    "\n",
    "The key idea behind equalization is to ensure that each pair of words is equally distant from the 49-dimensional space $g_\\perp$, which is orthogonal to the gender direction $g$. This process guarantees that gender-specific pairs are not skewed towards one gendered direction relative to neutral terms. In the figure below, you can visualize how equalization works in the embedding space.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    " <img src=\"https://raw.githubusercontent.com/umermjd11/DLC5M2A1/master/images/equalize10.png\" style=\"width:800px;height:400px;\">\n",
    "  <br>\n",
    "  <caption>\n",
    "    <font color=\"#00796b\" style=\"font-size: 1.2em; font-weight: bold;\">\n",
    "      <b>Figure 3</b>: Illustration of the equalization process for a pair of gendered words in relation to a neutral term.\n",
    "    </font>\n",
    "  </caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "The mathematical derivation of this algorithm is somewhat complex; for further details, refer to [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520). However, the key equations needed to implement the equalization algorithm are as follows:\n",
    "\n",
    "1. Calculate the mean vector $\\mu$ of the pair:\n",
    "   $$\n",
    "   \\mu = \\frac{e_{w1} + e_{w2}}{2}\n",
    "   $$\n",
    "\n",
    "2. Project $\\mu$ onto the bias axis to get $\\mu_B$:\n",
    "   $$\n",
    "   \\mu_{B} = \\frac {\\mu \\cdot \\text{bias\\_axis}}{||\\text{bias\\_axis}||_2^2} * \\text{bias\\_axis}\n",
    "   $$\n",
    "\n",
    "3. Calculate the component of $\\mu$ orthogonal to the bias axis, $\\mu_{\\perp}$:\n",
    "   $$\n",
    "   \\mu_{\\perp} = \\mu - \\mu_{B}\n",
    "   $$\n",
    "\n",
    "4. Project $e_{w1}$ and $e_{w2}$ onto the bias axis to get $e_{w1B}$ and $e_{w2B}$:\n",
    "   $$\n",
    "   e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias\\_axis}}{||\\text{bias\\_axis}||_2^2} * \\text{bias\\_axis}\n",
    "   $$\n",
    "   $$\n",
    "   e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias\\_axis}}{||\\text{bias\\_axis}||_2^2} * \\text{bias\\_axis}\n",
    "   $$\n",
    "\n",
    "5. Adjust $e_{w1B}$ and $e_{w2B}$ to make them equidistant from $\\mu_B$:\n",
    "   $$\n",
    "   e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{w1B} - \\mu_B}{||(e_{w1} - \\mu_{\\perp}) - \\mu_B||}\n",
    "   $$\n",
    "   $$\n",
    "   e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{w2B} - \\mu_B}{||(e_{w2} - \\mu_{\\perp}) - \\mu_B||}\n",
    "   $$\n",
    "\n",
    "6. Finally, construct the equalized vectors $e_1$ and $e_2$:\n",
    "   $$\n",
    "   e_1 = e_{w1B}^{corrected} + \\mu_{\\perp}\n",
    "   $$\n",
    "   $$\n",
    "   e_2 = e_{w2B}^{corrected} + \\mu_{\\perp}\n",
    "   $$\n",
    "\n",
    "The function implemented below use the equations provided to compute the equalized versions of a pair of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns:\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select word vector representation of \"word\"\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Step 2: Compute the mean of e_w1 and e_w2\n",
    "    mu = (e_w1 + e_w2) / 2\n",
    "\n",
    "    # Step 3: Project mu onto the bias axis to get mu_B\n",
    "    mu_B = (np.dot(mu, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "\n",
    "    # Step 4: Calculate the component of mu orthogonal to the bias axis (mu_perp)\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Step 5: Project e_w1 and e_w2 onto the bias axis to get e_w1B and e_w2B\n",
    "    e_w1B = (np.dot(e_w1, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "    e_w2B = (np.dot(e_w2, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "\n",
    "    # Step 6: Adjust e_w1B and e_w2B to make them equidistant from mu_B\n",
    "    norm_mu_orth = np.linalg.norm(mu_orth)\n",
    "    e_w1B_corrected = np.sqrt(np.abs(1 - norm_mu_orth**2)) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B)\n",
    "    e_w2B_corrected = np.sqrt(np.abs(1 - norm_mu_orth**2)) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B)\n",
    "\n",
    "    # Step 7: Construct the equalized vectors e_1 and e_2\n",
    "    e1 = e_w1B_corrected + mu_orth\n",
    "    e2 = e_w2B_corrected + mu_orth\n",
    "    \n",
    "    return e1, e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336832\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.35666618846270376\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.7004364289309387\n",
      "cosine_similarity(e2, gender) =  0.7004364289309387\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These debiasing algorithms provide effective methods for reducing gender bias in word embeddings. However, it is important to note that they are not perfect and cannot completely eliminate all traces of bias.\n",
    "\n",
    "One limitation of this implementation is that the bias direction $g$ was defined solely using the pair of words *woman* and *man*. As discussed earlier, defining $g$ by averaging over multiple word pairs—such as $g_1 = e_{woman} - e_{man}$, $g_2 = e_{mother} - e_{father}$, $g_3 = e_{girl} - e_{boy}$, and so forth—would yield a more accurate representation of the \"gender\" dimension within the 50-dimensional embedding space. By incorporating additional gendered word pairs, we can create a more robust estimate of the bias direction, potentially improving the effectiveness of the debiasing process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">6. Conclusion</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have reached the conclusion of this notebook, where we have explored various applications and modifications of word vectors. Through this process, we have gained insights into how these representations can be leveraged and adapted for different tasks in natural language processing.\n",
    "\n",
    "We commend the progress made and encourage continued exploration in this field.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Cosine Similarity**: This is an effective metric for comparing the similarity between pairs of word vectors.  \n",
    "    - Additionally, L2 (Euclidean) distance can also be used for measuring vector similarity.\n",
    "- **Pre-trained Word Vectors**: For many NLP applications, starting with a pre-trained set of word vectors is advantageous and often yields better initial results.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "As word embeddings continue to evolve, several avenues for future research and improvement emerge:\n",
    "\n",
    "- **Contextual Word Embeddings**: While traditional word embeddings map each word to a single vector, advancements like contextual embeddings (e.g., BERT, GPT) provide word vectors that change depending on the surrounding context. Further research could explore how to improve the efficiency and scalability of these models for various NLP tasks.\n",
    "- **Multilingual Word Embeddings**: Expanding word embeddings to better handle multiple languages simultaneously is an ongoing area of development. Future work could explore better ways to create embeddings that represent semantic relationships across languages, enabling cross-lingual tasks.\n",
    "- **Domain-Specific Embeddings**: Custom embeddings trained on specialized corpora, such as medical or legal texts, hold potential for improving task performance in these fields. Further exploration into fine-tuning word vectors for domain-specific applications could lead to more accurate models.\n",
    "- **Ethical Considerations**: The embedding models often carry biases present in the training data. Future directions in this area could focus on mitigating these biases, ensuring that word embeddings are fair, unbiased, and socially responsible.\n",
    "\n",
    "We look forward to the continued development of these techniques and their application to a wide range of NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">7 - References</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
